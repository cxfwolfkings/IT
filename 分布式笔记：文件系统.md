# 目录

- Linux文件系统的进化
- 传统的分布式文件系统
- 新型分布式文件系统
- 互联网领域中的小文件系统



## 新型分布式文件系统

### HDFS

在Hadoop生态系统中，Hadoop分布式文件系统(HDFS)是非常关键的一环，它为管理大数据资源池和支撑相关大数据分析应用提供了一个具有高可靠性的工具。在分布式存储领域，HDFS也扮演着重要角色，是作为系统架构师必须了解的分布式文件系统之一。

![x](http://viyitech.cn/public/images/hdfs.png)

**HDFS的工作原理**

HDFS支持在计算节点之间快速传输数据。在开始阶段，它与MapReduce紧密耦合——MapReduce是一个用于大规模数据集的并行运算的编程框架。

当HDFS接收数据时，会将信息分解为单独的块，并将它们分布到集群中的不同节点，从而支持高效的并行处理。

此外，HDFS专门设计有高容错的特性。HDFS可以多次复制每个数据片段，并将副本分发给各个节点，将至少一个副本放在其他服务器机架上。因此，在集群中的其他地方也可以找到崩溃节点上的数据。这确保了在恢复数据时可以继续进行处理。

HDFS使用主/从架构。在其最初版本中，每个Hadoop集群由一个NameNode（用于管理文件系统运行）和支持DataNode（用于管理单个计算节点上的数据存储）组成。这些HDFS元素结合起来，可支持具有大数据集的应用程序。

这个主节点“数据分块”架构，从谷歌文件系统(GFS)以及IBM的通用并行文件系统(GPFS)中吸取了部分设计指导元素。GFS是一个可扩展的分布式文件系统，用于大型的、分布式的、对大量数据进行访问的应用。它运行于廉价的普通硬件上，并提供容错功能，可以给大量的用户提供总体性能较高的服务。GPFS是一种专门为集群环境设计的高性能、可扩展的并行文件系统，可以在集群中的多个节点间实现对共享文件系统中文件的快速存取操作，并提供稳定的故障恢复和容错机制。此外，虽然HDFS不是与可移植操作系统接口(POSIX)的模型兼容的，但它在某些方面也与POSIX设计风格相呼应。

HDFS架构图-应用程序通过Client与NameNode和DataNode进行交互

![x](http://viyitech.cn/public/images/hdfs_arch.jpg)

**为什么要使用HDFS？**

HDFS最早出自雅虎，用于满足该公司广告服务和搜索引擎的部分需求。与其他面向Web的公司一样，雅虎发现自己需要处理的应用程序的用户访问越来越多，而这些用户正在产生越来越多的数据。后来，Facebook、eBay和Twitter等公司也开始使用HDFS作为大数据分析的基础，来解决相同的需求。

但HDFS的用途远不止于此。上述所提到的大规模Web搜索，我们可以将其归类于数据密集型并行计算。此外，HDFS也常用于计算密集型并行计算应用场景，如气象计算。在数据密集与计算密集混合型场景中也多有应用，如3D建模与渲染。HDFS也是许多开源数据仓库（有时称为数据湖，Data Lake）的核心。

HDFS通常用于大规模部署，因为它具备一个重要特性，就是可运行在普通廉价机器上。还有，例如运行Web搜索和相关应用程序的这种系统，往往需要可以扩展到数百PB和数千个节点，因此系统必须拥有易于扩展的特性，这正是HDFS所具备的。此外，服务器故障在这种规模下很常见，HDFS所提供的容错能力在这方面也很有实际价值。

**不适用于HDFS的场景**

首先，HDFS不适用于对延迟要求较高的场景，如实时查询。在延迟方面，HDFS没有充分的优势。其次，HDFS也难以支持大量小文件的存储。在Hadoop系统中，通常将“小文件”定义为远小于HDFS的block size（默认64MB）的文件，由于每个文件都会产生各自的MetaData元数据，Hadoop通过Namenode来存储这些信息，若小文件过多，容易导致占用大量NameNode内存，也会使寻道时间超过读取时间，为系统带来性能瓶颈。

此外，HDFS不支持多用户写入，也无法进行文件随机修改。仅支持以append的方式，即通过追加的方式添加到文件结尾处。HDFS适用于存储半结构化和非结构化数据，若有数据严格的结构化特性，强行采用HDFS是不合适的。最后，HDFS适用于TB、PB级的大数据处理，文件数量通常在百万以上，如果数据量很小，完全没有必要采用HDFS。

**HDFS和Hadoop的历史**

这里简要说一些关键的时间节点。2006年，Apache Hadoop项目正式启动，HDFS和MapReduce开始独立发展。该软件开始广泛应用于各行各业的大数据分析项目中。2012年，HDFS和Hadoop版本1.0发布。

2013年Hadoop 2.0版本加入了通用YARN资源管理器，MapReduce和HDFS有效解耦。此后，Hadoop支持各种数据处理框架和文件系统。虽然MapReduce经常被Apache Spark所替代，但HDFS仍然是Hadoop的一种流行的文件格式。

在发布了四个alpha版本和一个beta版本之后，Apache Hadoop 3.0.0在2017年12月开始普遍可用，HDFS增强支持额外的NameNode、擦除编码工具和更大的数据压缩。与此同时，HDFS工具，如LinkedIn的开源Dr. Elephant和Dynamometer 性能测试工具的进步，也使HDFS能够支持更进一步的开发实现。

## 互联网领域中的小文件系统



### MooseFS

前一段公司因为nfs存储的问题，做了一段时间的调研。最终虽然有nfs高可用方案(nfs+drbd+heartbeat)，但是在nfs故障切换的时候会出现 2 分钟左右的延时。这样子。。。就开始了对分布式文件系统的调研选型。也就是这样，有了本系列的博文。

针对 MooseFS 预计会有 3 篇博文，分为介绍、部署、深入。本篇博文主要介绍 MooseFS 。

#### 一、简述
**1、介绍**

MooseFS是一个具备冗余容错功能的分布式网络文件系统，它将数据分别存放在多个物理服务器或单独磁盘或分区上，确保一份数据有多个备份副本。对于访问的客户端或者用户来说，整个分布式网络文件系统集群看起来就像一个资源一样。

从其对文件操作的情况看，MooseFS就相当于一个类UNIX文件系统:

- mfs是一个分层的目录树结构
- 存储支持POSIX标准的文件属性（权限，最后访问和修改时间）
- 支持特殊的文件，如：块设备，字符设备，管道和套接字，链接文件（符号链接和硬链接）
- 支持基于IP地址和密码的方式访问文件系统

**2、特性**

1、高可靠性，每一份数据可以设置多个副本（多份数据），并可以存储在不同的主机上
    2、高可扩展性，可以很轻松的通过增加主机磁盘容量或增加主机数量来动态扩展整个文件系统的存储量
    3、高可容错性，我们可以通过对mfs进行系统设置，实现当数据文件被删除后的一段时间内，依然存放于主机的回收站中，以备误删恢复数据
    4、高数据一致性，即便文件被写入/访问时，我们依然可以完成对文件的一致性快照

**3、优缺点**

优点：

- 由于MFS是基于GPL发布的，因此完全免费，并且开发和社区都很活跃，资料也非常丰富
- 轻量、易部署、易配置、易维护
- 通用文件系统，不需要修改上层应用就可以使用（那些需要专门 API 的DFS确实有点麻烦）
- 扩容成本低、支持在线扩容，不影响业务，体系架构可伸缩性极强（官方的case可以扩到70台了！）
- 体系架构高可用，所有组件无单点故障
- 文件对象高可用，可设置任意的文件冗余程度（提供比 Raid 10 更高的冗余级别）
- 提供系统负载，将数据读写分配到所有的服务器上，加速读写性能
- 提供诸多高级特性，比如类似Windows的回收站功能、类似JAVA语言的GC（垃圾回收）、快照功能等
- MooseFS 是 Google Filesystem 的一个 c 实现
- 自带 Web Gui 的监控接口
- 提高随机读或写效率和海量小文件的读写效率（有待进一步证明）

缺点：

- Master Server 本身的性能瓶颈。MFS的主备架构情况类似于MySQL的主从复制，从可以扩展，主却不容易扩展。短期的对策就是按照业务来做切分。
- 随着MFS体系架构中存储文件的总数上升，Master Server对内存的需求量会不断增大（MFS把文件系统的结构缓存到 Maset Server 的内存中）。根据官方提供的数据，8g对应2500w的文件数，2亿文件就得64GB内存。短期的对策也是按照业务来做切分。
- Master server的单点解决方案的健壮性。目前官方自带的是把数据信息从Master Server同步到Metalogger Server上，Master Server一旦出问题Metalogger Server可以恢复升级为Master Server，但是需要恢复时间。目前，也可以通过第三方的高可用方案（heartbeat+drbd+moosefs）来解决 Master Server 的单点问题。
- Metalogger Server 复制元数据的间隔时间较长（可调整）

**4、应用场景**

谈及MooseFS的应用场景，其实就是去谈分布式文件系统的应用场景。

- 大规模高并发的数据存储及访问（小文件、大文件），TFS适合小文件（<1M）
- 大规模的数据处理，如日志分析

**5、使用现状**

针对 MooseFS 在全球的使用情况，我在国内的一篇论文上挖了一张图。图片如下，在该图中标记为驯鹿的就是 Moosefs 的使用者所在区域。从图中可以看出，它在中国、欧洲以及北美都拥有了大量的用户，正是有了广泛的用户基础，才能使得mfs特性能够快速的迭代和进步。

![x](http://viyitech.cn/public/images/mfs_used.jpg)

MooseFS官方的使用情况链接：http://www.moosefs.org/who-is-using-moosefs.html

#### 二、组成

**1、架构图**

![x](http://viyitech.cn/public/images/mfs_arch.jpg)

整个架构中，主要有四个组件，分别是管理服务器 Master Server、备份服务器Metalogger Server、数据存储服务器 Chunk Server 和 客户端 Client。

- 其中，管理服务器 Master Server 负责所有数据存储服务器的数据存储管理，响应客户端文件的读写请求，收回文件空间以及恢复文件，多存储节点之间的文件复制；
- 元数据日志服务器 Metalogger Server，对Master Server服务器的变化日志文件进行备份，changelog_ml.*.mfs 是备份文件的类型，当 Master Server 出现故障时替换其继续工作，避免 Master Server 的单点故障导致分布式文件系统的不能正常运行；
- 数据存储服务器chunkserver，服从 Master Server 的安排，定期向 Master Server 发送自己的状态信息，除此之外，还能向客户提供数据存储空间，能够向客户传输数据；
- 客户端 Client，通过 FUSE 内核接口挂载到数据存储服务器上，在客户端看来使用数据存储服务器上的文件系统和使用本地Unix文件系统是一样的。

下面再针对这4个组件进行更详细的介绍!

**2、四个组件**

| **组件名称**                                                 | **组件作用**                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 管理服务器  Managing Server  简称Master  Server              | 这个组件的角色是管理整个mfs文件系统的主服务器，除了分发用户请求外，还用来存储整个文件系统中的每个数据文件的metadata信息，metadata（元数据）信息包括文件（也可以是目录、socket、管道、设备等）的大小、属性、文件位置路径等，以及文件空间的回收和恢复，控制多chunk  server节点的数据拷贝。很类似lvs负载均衡主服务器，不同的是lvs仅仅根据算法分发请求，而master根据内存里的metadata信息来分发请求。这个master只能有一台处于激活工作的状态。 |
| 元数据备份服务器  Metadata  backup servers  简称metalogger Server | 这个组件的作用是备份管理服务器master的变化的metadata信息日志文件，文件类型为changelog_ml.*.mfs，以便于在主服务器出现问题的时候，可以经过简单的操作即可让新主服务器进行工作。这很类似Mysql的主从同步，只不过他不像mysql从库那样在本地应用数据，而只是接收主服务器上文件写入时记录的文件相关的metadata信息。这个backup可以有一台或多台，它很类似于lvs从负载均衡器。 |
| 数据存储服务器组  Data Servers  简称Chunk  Servers           | 这个组件就是真正存放数据文件实体的服务器了，这个角色可以有多台不同的物理服务器或不同的磁盘及分区来充当，当配置数据的副本多于一份时，即写入到一个数据服务器后，会根据算法在其他数据服务器上进行同步备份。这个很像lvs集群的rs节点。 |
| 客户机服务器组  Client                                       | 这个组件就是挂载并使用mfs文件系统的客户端，当读写文件时，客户端首先连接主管理服务器获取数据的metadata信息，然后根据得到的metadata信息，访问数据服务器读取或写入文件实体。mfs客户端通过FUSE  mechanism实现挂载MFS文件系统的。因此，只要系统支持FUSE，就可以作为客户端访问MFS整个文件系统。所谓的客户端并不是网站用户，而是前端访问文件系统的应用服务器，如web |

#### 三、原理

MooseFS是一个文件系统，因此其最主要的工作就是文件的读写流程，即 MooseFS的主要的两大工作过程。

**1、MooseFS的读文件流程：**

首先要了解的是它的读过程。如下图所示：

![x](http://viyitech.cn/public/images/mfs_read.jpg)

图中 Master Server用三角形表示，Chunk Server用圆形表示，Client 用方形表示。整个读过程有以下四个步骤：

1. 首先client客户端访问主服务器master，获取文件实体的位置等相关信息
2. 主服务器master查询缓存记录，把文件实体的位置等相关信息发给client客户端
3. Client客户端根据拿到的信息去访问对应的存储实体数据的服务器（data servers或者 chunk servers）
4. 存储实体数据的服务器（data servers或者 chunk servers）把对应的数据返回给Client客户端

从上图，我们还可以看出，当多个MFS客户端读数据的时候，master服务器充当路由为这些客户端分发指路的作用，而数据的返回时由不同的数据服务器直接返回给请求的客户端，这样的模式可以极大的减轻主服务器的系统及网络瓶颈，增加了整个系统的吞吐，很像LVS的DR模式的负载均衡的分发和数据传输的情况。

**2、MooseFS的写文件流程：**

相对于读文件，MooseFS的写文件流程较为复杂，如图所示：

![x](http://viyitech.cn/public/images/mfs_write.jpg)

图中 Master Server用三角形表示，Chunk Server用圆形表示，Client 用方形表示。整个写过程要分为下面八个步骤：

1. Client客户端访问主服务器master，请求写入数据
2. 主服务器master查询缓存记录，如果是新文件，则会联系后面的数据服务器创建对应的chunk对象准备存放文件。
3. 数据服务器返回创建对应的chunk对象成功给主服务器
4. 主服务master把文件实体的位置等相关信息发给client客户端。
5. Client客户端访问对应的数据服务器写数据
6. 数据服务器之间进行数据同步，互相确认成功
7. 数据服务器返回成功写入信息给Client客户端
8. Client客户端回报给主服务器master写入结束

OK！以上就是 MooseFS 的简单介绍，希望能对51博友有所帮助！

本文出自 "Not Only Linux" 博客，请务必保留此出处http://nolinux.blog.51cto.com/4824967/1600890

上篇博文对 MooseFS 的结构，组件和特性做了相关介绍。那么，本篇博文自然就是如何去部署实施 MooseFS 这样一个分布式文件系统喽。废话不多说，下面就是整个部署流程！

#### 四、架构规划

由于在整个MooseFS的架构中，默认是一台 Master，一台 Metalogger ，三台 Chunk Servers。因此，在本次实施规划当中，我们就使用5台虚拟机来进行整个架构的实施部署。

其中 IP地址 的规划情况如下：

- 元数据服务器           mfs-master-1                 172.16.100.2
- 备份服务器               mfs-metalogger             172.16.100.4
- 数据存储服务器       mfs-chunkserver-1        172.16.100.5
- 数据存储服务器       mfs-chunkserver-2        172.16.100.6
- 数据存储服务器       mfs-chunkserver-3        172.16.100.7

由于这次测试空闲服务器不够，因此采用了虚拟机进行测试（好蛋疼）。但是在实际生产环境中，针对每个组件的服务器选型问题，还是必须要注意的。这里提供如下几条建议：

1、Master Server

由于 Master Server 控制着整个 MooseFS 中的各个组件，并且负责对外提供服务，因此我们一定需要保证 Master Server 处于非常稳定的状态。比如，针对 Master Server采用双电源双路配置，多块磁盘使用RAID1或RAID10，进行冗余。

前面也提到，Master Server 将所有访问的元数据信息都放在内存当中，提供用户访问。因此，当文件数量增加的时候，内存使用量也会增加。根据官方的数据，100万个文件chunk信息，大概需要300M的内存空间来进行。对于磁盘来讲，Master Server 对磁盘的使用量不是很大，这个取决于所用的文件和chunk块的数目（记录在主元数据文件）以及对文件作出操作的数量（记录在元数据更改日志），一般情况下 20G 可以用来存储信息 2500 万个文件变更记录长达50小时。由此看来，作为Master Server 内存量够大才是重中之重。

2、Metalogger Server

在 MooseFS 的设计中，虽然 Metalogger Server 只是用来收集 MooseFS 主服务器的元数据（文件更改的信息的变化）的备份，对硬件的要求不应该高于主服务器的备份。但是需要注意的是，如果在Master Server没有做高可用的情况下，主服务器宕机之后，我们是需要启用Metalogger Server 来顶替主服务器的。因此，从这方面考虑，Metalogger Server 至少也是和 Master Server 的配置相同的，这点切记！

3、Chunk Server

针对 Chunk Server，它是真正存储数据的载体。因此，我们对它的要求就简单粗暴了很多，只要保障硬盘的性能即可。如果是普通的业务，可以选择多块盘做RAID5即可，当然RAID0或RAID10都是可以的。

需要注意的是，由于 MooseFS 的默认负载均衡算法的问题，我建议所有 Chunk Server 的磁盘大小保持一致。这样子，我们才能保证 MooseFS 在使用过程中，各个 Chunk 节点的数据使用量是大致一致的。否则，磁盘容量大的 Chunk Server 使用量会加大，而磁盘容量小的 Chunk Server 的使用量会变小。切记，切记！

当然，如果公司员工有能力的话，也可以对 MooseFS 的负载均衡算法中每次对carry 变量的增加算法这一部分进行改进，来避免默认算法的缺点，使存储数据能够均衡分布在各个 Chunk Server 上。

**五、部署 Master Server**

**1、编译参数介绍**

我们已经知道 MooseFS是由 4 个组件组成的，但是 MooseFS 提供的安装包却只有一个，因此针对每个组件的不同设置，都需要我们在编译之前的配置阶段进行配置，比如安装 Master Server 的时候，我们需要使用 --disable-mfschunkserver和--disable-mfsmount参数；安装 Chunk Server 的时候，我们需要使用 --disable-mfsmaster参数；安装 Client 时，我们需要使用--disable-mfsmaster和--disable-mfschunkserver参数；如果是安装 Metalogger Server，我们使用和安装 Master Server时的参数一样即可。

下面，在部署 Master Server 之前，在详细介绍下 MooseFS 安装包的几个关键的配置参数：

```sh
--disable-mfsmaster                  # 不创建成管理服务器（用于纯节点的安装）
--disable-mfschunkserver             # 不创建成数据存储chunkserver服务器 
--disable-mfsmount                   # 不创建mfsmount和mfstools（如果用开发包安装，他们会被默认创建的）
--enable-mfsmount                    # 确定安装mfsmount和mfstools（如果
--prefix=DIRECTORY                   # 锁定安装目录（默认是/usr/local）
--sysconfdir=DIRECTORY               # 选择配置文件目录（默认是${prefix}/etc)）
--localstatedir=DIRECTORY            # 选择变量数据目录（默认是${prefix}/var，MFS元数据被存储在mfs的子目录下，默认是${prefix}/var/mfs ）
--with-default-user                  # 运行守护进程的用户，如果配置文件中没有设定用户，默认为nobody用户
--with-default-group                 # 运行守护进程的用户组，如果配置文件中没有设定用户组，默认为nogroup用户组
```

**2、安装 Master Server**

当我们安装主服务器mfs master时，由于我们的目的是安装主服务器，因此在配置过程中（./configure），可以选择加参数取消安装 Chunk server（使用--disable-mfschunkserver）以及 MooseFS 客户端（使用--disable-mfsmount）。

下面，列出安装 MooseFS 主控服务器 Master Server 的详细步骤：

```sh
yum install zlib-devel -y
groupadd -g 1001 mfs
useradd -u 1001 -g mfs -s /sbin/nologin mfs
cd /usr/local/src
wget http://moosefs.org/tl_files/mfscode/mfs-1.6.27-5.tar.gz
tar zxf mfs-1.6.27-5.tar.gz
cd mfs-1.6.27
./configure --prefix=/usr/local/mfs-1.6.27 --with-default-user=mfs --with-default-group=mfs  --disable-mfschunkserver --disable-mfsmount
make
make install
ln -s /usr/local/mfs-1.6.27 /usr/local/mfs 
ll -d /usr/local/mfs 
# lrwxrwxrwx. 1 root root 21 Dec 28 23:28 /usr/local/mfs -> /usr/local/mfs-1.6.27
cd
```

这里再补一份快捷操作命令：

```sh
yum install zlib-devel -y
groupadd -g 1000 mfs
useradd -u 1000 -g mfs mfs -s /sbin/nologin
cd /usr/local/src && wget http://moosefs.org/tl_files/mfscode/mfs-1.6.27-5.tar.gz
tar zxf mfs-1.6.27-5.tar.gz && cd mfs-1.6.27
./configure --prefix=/usr/local/mfs-1.6.27 --with-default-user=mfs --with-default-group=mfs --disable-mfschunkserver --disable-mfsmount
make && make install
ln -s /usr/local/mfs-1.6.27 /usr/local/mfs
ll -d /usr/local/mfs 
cd
```

以上就是 MooseFS 的 Master Server 的安装过程。Master Server 安装成功以后，会在安装目录 `/usr/local/mfs-1.6.27`下生成几个目录文件。其中etc目录为mfs配置文件目录，里面有很多默认配置文件样例（结尾后缀为*.dist），后续我们将借用这些样例作为 MFS 服务器的目标配置文件。

这里来看下每个目录的用途：

```sh
# ll /usr/local/mfs/ 
total 16 
drwxr-xr-x. 3 root root 4096 Dec 29 00:10 etc       # MFS 的配置文件目录，里面放了很多模板文件 
drwxr-xr-x. 2 root root 4096 Dec 29 00:10 sbin      # MFS 命令路径
drwxr-xr-x. 4 root root 4096 Dec 29 00:10 share     # MFS 帮助文件目录
drwxr-xr-x. 3 root root 4096 Dec 29 00:10 var       # MFS 数据及日志目录，例如：metadata 数据
```

提示：etc和var需要备份。

注意：etc和var目录里面存放的是配置文件和MFS的数据结构信息，因此请及时做好备份，防止灾难损毁。后面做了 Master Server双机之后，就可以解决这个问题。

**3、配置 Master Server**

上面简单看了 MooseFS 的目录结构，其中 Master Server 的配置文件是位于/MooseFS根目录/etc 目录下。在该目录下有很多模板配置文件，包括 Master，Metalogger、ChunkServer等，有关 Master 的配置文件主要有两个，一个是 mfsmaster.cfg，另一个是 mfsexports.cfg。其中，mfsmaster.cfg 是主配置文件，mfsexports.cfg 用来指定那些客户端主机可以远程挂载 MooseFS 文件系统以及赋予挂载客户端什么样的访问权限。

现在，我们去掉 Master 相关的两个配置文件的注释。

需要注意的是，这里一定要使用 cp，而不是 mv。这样，在操作之前保留原文件，一方面方便更改后对比变化，另一方面也修改出错后的回滚。

下面列出配置步骤以及每个配置文件的详解！

```sh
/usr/local/mfs/etc/mfs
cp mfsexports.cfg.dist mfsexports.cfg 
cp mfsmaster.cfg.dist mfsmaster.cfg
```

这里我们先来看下主配置文件mfsmaster.cfg

```sh
[root@mfs-master-1 ~]# cat /usr/local/mfs/etc/mfs/mfsmaster.cfg 
# WORKING_USER = mfs         # 运行 master server 的用户
# WORKING_GROUP = mfs        # 运行 master server 的组
# SYSLOG_IDENT = mfsmaster   # 是master server在syslog中的标识，也就是说明这是由master serve产生的
# LOCK_MEMORY = 0            # 是否执行mlockall()以避免mfsmaster 进程溢出（默认为0）
# NICE_LEVEL = -19           # 运行的优先级(如果可以默认是 -19; 注意: 进程必须是用root启动)
# EXPORTS_FILENAME = /usr/local/mfs-1.6.27/etc/mfs/mfsexports.cfg   # 被挂载目录及其权限控制文件的存放路径
# TOPOLOGY_FILENAME = /usr/local/mfs-1.6.27/etc/mfs/mfstopology.cfg # mfstopology.cfg文件的存放路径
# DATA_PATH = /usr/local/mfs-1.6.27/var/mfs # 数据存放路径，此目录下大致有三类文件，changelog，sessions和stats；
# BACK_LOGS = 50             # metadata的改变log文件数目(默认是 50)
# BACK_META_KEEP_PREVIOUS = 1     # metadata的默认保存份数（默认为1）
# REPLICATIONS_DELAY_INIT = 300   # 延迟复制的时间（默认是300s）
# REPLICATIONS_DELAY_DISCONNECT = 3600   # chunkserver断开的复制延迟（默认是3600）
# MATOML_LISTEN_HOST = *          #  metalogger监听的IP地址(默认是*，代表任何IP)
# MATOML_LISTEN_PORT = 9419       # metalogger监听的端口地址(默认是9419)
# MATOML_LOG_PRESERVE_SECONDS = 600
# MATOCS_LISTEN_HOST = *          # 用于chunkserver连接的IP地址（默认是*，代表任何IP）
# MATOCS_LISTEN_PORT = 9420       # 用于chunkserver连接的端口地址（默认是9420）
# MATOCL_LISTEN_HOST = *          # 用于客户端挂接连接的IP地址(默认是*，代表任何IP)
# MATOCL_LISTEN_PORT = 9421       # 用于客户端挂接连接的端口地址（默认是9421）
# CHUNKS_LOOP_MAX_CPS = 100000    # chunks的最大回环频率（默认是：100000秒）
# CHUNKS_LOOP_MIN_TIME = 300      # chunks的最小回环频率（默认是：300秒）
# CHUNKS_SOFT_DEL_LIMIT = 10      # 一个chunkserver中soft最大的可删除数量为10个
# CHUNKS_HARD_DEL_LIMIT = 25      # 一个chuankserver中hard最大的可删除数量为25个
# CHUNKS_WRITE_REP_LIMIT = 2      # 在一个循环里复制到一个chunkserver的最大chunk数目（默认是1）
# CHUNKS_READ_REP_LIMIT = 10      # 在一个循环里从一个chunkserver复制的最大chunk数目（默认是5）
# ACCEPTABLE_DIFFERENCE = 0.1     # 每个chunkserver上空间使用率的最大区别（默认为0.01即1%）
# SESSION_SUSTAIN_TIME = 86400    # 客户端会话超时时间为86400秒，即1天
# REJECT_OLD_CLIENTS = 0          # 弹出低于1.6.0的客户端挂接（0或1，默认是0）
 
# deprecated: 
# CHUNKS_DEL_LIMIT - use CHUNKS_SOFT_DEL_LIMIT instead 
# LOCK_FILE - lock system has been changed, and this option is used only to search for old lockfile
```

下面附上官方地址：http://moosefs.com/Content/Downloads/moosefs-users-manual.pdf

该配置文件默认全部都为注释。mfs官方默认这样规定的，每一个注释的配置都是mfs此项配置的默认值。其实，我们linux中很多配置文件的设计风格都是这样子的，比如sshd_config.如果打算更改这些配置文件中某些项的参数，只需要取消注释，并且更改对应的参数即可。

针对master来讲，mfsmaster.cfg的默认配置无需更改即可投入使用。

下面，就是权限控制配置文件mfsexports.cfg。

该配置文件制定了哪些客户端可以远程挂接MFS文件系统，以及授予挂载客户端什么样的访问权限。例如，我们制定只有172.16.0.0/24网段的主机可以读写模式访问MFS的整个共享结构资源（/）。在配置文件mfsexports.cfg中写入如下信息即可。

```sh
[root@mfs-master-1 ~]# cat /usr/local/mfs/etc/mfs/mfsexports.cfg
*    /    rw,alldirs,mapall=mfs:mfs,password=redhat
*    .    rw  # 如果需要使用 moosefs的回收站功能，请开启此行。如果不使用，关闭即可
```

这里，我列出该文件的配置规范：

mfsexports.cfg 文件中，每一个条目就是一个配置规则，而每一个条目又分为三个部分，其中第一部分是mfs客户端的ip地址或地址范围，第二部分是被挂载的目录，第三个部分用来设置mfs客户端可以拥有的访问权限。

第一部分：mfs客户端的ip地址或地址范围。地址可以指定的几种表现形式：

```sh
    *                  所有的 IP 地址
    n.n.n.n            单个 IP 地址
    n.n.n.n/b          IP 网络地址/位数掩码
    n.n.n.n/m.m.m.m    IP 网络地址/子网掩码
    f.f.f.f-t.t.t.t    IP 段
```

第二部分：被挂载的目录。目录部分需要注意两点：

```sh
 /   标识MooseFS根
 .   表示MFSMETA文件系统
```

第三部分：设置mfs客户端可以拥有的访问权限。权限部分：

```sh
ro        只读模式共享
rw        读写模式共享
alldirs   允许挂载任何指定的子目录
maproot   映射为root，还是指定的用户
password  指定客户端密码
```

OK，以上两个配置文件配置完毕之后，我们还需要注意一个文件。在 Master Server 首次安装之后，会在 /usr/local/mfs/var/mfs/ 目录下生成一个名为 metadata.mfs.empty 的元数据metadata文件，该文件默认是为空的。（当你整个MooseFS配置好之后，它就有数据了）

Master Server 的运行必须有metadata.mfs，而这个文件就是从metadata.mfs.empty更名而来的。当然，这个更名操作需要我们自己手动来完成。操作如下：

```sh
cp /usr/local/mfs/var/mfs/metadata.mfs.empty /usr/local/mfs/var/mfs/metadata.mfs
```

**4、启动 Master Server**

在 MooseFS 的架构中，Master Server 是不依附于其它几个组件的，它可以单独启动。但是需要注意，其它组件必须要等 Master Server 起来之后才能启动！切记！！！

在通过上面的简单配置之后，我们就可以启动 Master Server 了！下面列出启动和检查过程。

```sh
/usr/local/mfs/sbin/mfsmaster start    # 启动mfs主服务器
```

输出：

```sh
working directory: /usr/local/mfs-1.6.27/var/mfs 
lockfile created and locked 
initializing mfsmaster modules ... 
loading sessions ... file not found 
if it is not fresh installation then you have to restart all active mounts !!! 
exports file has been loaded 
mfstopology configuration file (/usr/local/mfs-1.6.27/etc/mfstopology.cfg) not found - using defaults 
loading metadata ... 
create new empty filesystemmetadata file has been loaded 
no charts data file - initializing empty charts 
master <-> metaloggers module: listen on *:9419 
master <-> chunkservers module: listen on *:9420 
main master server module: listen on *:9421 
mfsmaster daemon initialized properly
```

下面进行检查，针对 Master Server 的启动检查主要有3个方面，**第一看进程，第二看端口，第三看日志**。

```sh
[root@mfs-master-1 ~]# ps -ef|grep mfs            # 查看进程是否正常
mfs 28867 1 0 19:44 ? 00:00:00 /etc/ha.d/resource.d/mfsmaster start 
root 29087 26249 0 19:48 pts/1 00:00:00 grep mfs
[root@mfs-master-1 ~]# netstat -lnt |grep 94      # 查看端口看是否起来
tcp 0 0 0.0.0.0:9419 0.0.0.0:* LISTEN 
tcp 0 0 0.0.0.0:9420 0.0.0.0:* LISTEN 
tcp 0 0 0.0.0.0:9421 0.0.0.0:* LISTEN
```

开启 master 的日志情况

```sh
[root@mfs-master-1 ~]# tailf /var/log/messages    # 观察 Master Server 启动时的日志变化
Dec 31 19:52:10 mfs-master-1 mfsmaster[29112]: set gid to 1000
Dec 31 19:52:10 mfs-master-1 mfsmaster[29112]: set uid to 1000
Dec 31 19:52:10 mfs-master-1 mfsmaster[29112]: sessions have been loaded
Dec 31 19:52:10 mfs-master-1 mfsmaster[29112]: exports file has been loaded
Dec 31 19:52:10 mfs-master-1 mfsmaster[29112]: mfstopology configuration file (/usr/local/mfs-1.6.27/etc/mfstopology.cfg) not found - network topology not defined
Dec 31 19:52:10 mfs-master-1 mfsmaster[29112]: stats file has been loaded
Dec 31 19:52:10 mfs-master-1 mfsmaster[29112]: master <-> metaloggers module: listen on *:9419
Dec 31 19:52:10 mfs-master-1 mfsmaster[29112]: master <-> chunkservers module: listen on *:9420
Dec 31 19:52:10 mfs-master-1 mfsmaster[29112]: main master server module: listen on *:9421
Dec 31 19:52:10 mfs-master-1 mfsmaster[29112]: open files limit: 5000
```

**5、停止 Master Server**

Master Server 服务和其它普通服务不一样，它千万不能使用 `kill -9` 去强制杀掉进程。每次非正常关闭服务，都需要使用 MooseFS 自带的恢复工具进行数据恢复，非常悲剧。因此，安全的关闭 Master Server 是非常重要。

```sh
[root@mfs-master-1 ~]# /usr/local/mfs/sbin/mfsmaster stop 
sending SIGTERM to lock owner (pid:29112) waiting for termination ... terminated
[root@mfs-master-1 ~]# tailf /var/log/messages    # 观察 Master Server 关闭时的日志变化
Dec 31 19:53:16 mfs-master-1 mfsmaster[29113]: set gid to 1000
Dec 31 19:53:16 mfs-master-1 mfsmaster[29113]: set uid to 1000
Dec 31 19:53:16 mfs-master-1 mfsmaster[29112]: terminate signal received
Dec 31 19:53:16 mfs-master-1 mfsmaster[29112]: main master server module: closing *:9421
Dec 31 19:53:16 mfs-master-1 mfsmaster[29112]: master <-> chunkservers module: closing *:9420
Dec 31 19:53:16 mfs-master-1 mfsmaster[29112]: master <-> metaloggers module: closing *:9419
```

**6、扫尾操作**

a、配置环境变量

为了方便操作 MooseFS，我们需要把他的sbin目录加入到系统的PATH变量中去。

```sh
echo '# add moosefs to the path variable' >> /etc/profile 
echo 'PATH=/usr/local/mfs/sbin/:$PATH' >> /etc/profile 
tail -2 /etc/profile # add moosefs to the path variable PATH=/usr/local/mfs/sbin/:$PATH
source /etc/profile
```

b、配置开机自启动

开机自启动的目的，这里就不多解释了。

```sh
echo '# Configure the metalogger service startup' >> /etc/rc.local 
echo '/usr/local/mfs/sbin/mfsmetalogger start' >> /etc/rc.local 
tail -2 /etc/rc.local 
# Configure the metalogger service startup 
/usr/local/mfs/sbin/mfsmetalogger start
```

以上，就是 Master Server 部署的全部操作。

#### 六、部署 Metalogger Server

1、安装 Metalogger Server

前面已经介绍了，Metalogger Server 是 Master Server 的备份服务器。因此，Metalogger Server 的安装步骤和 Master Server 的安装步骤相同。并且，最好使用和 Master Server 配置一样的服务器来做 Metalogger Server。这样，一旦主服务器master宕机失效，我们只要导入备份信息changelogs到元数据文件，备份服务器可直接接替故障的master继续提供服务。这里仅列出安装步骤：

