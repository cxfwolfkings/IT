# 中间件

## 目录

- 定时调度
  - 单机定时调度
  - 分布式定时调度
- RPC
  - Dubbo
  - Thrift
  - gRPC
- 数据库中间件
  - Sharding Jdbc
  - [Mycat](#Mycat)
- 日志系统
  - 日志搜集
- [配置中心](#配置中心)
  - [基于zookeeper实现统一配置管理](#基于zookeeper实现统一配置管理)
- 服务注册中心
  - [Eureka](#Eureka)
  - [Zookeeper](#Zookeeper)
  - [Consul](#Consul)
  - [服务注册中心选型](#服务注册中心选型)
- [参考](#参考)



## Mycat

**什么是MyCAT？**

简单的说，MyCAT就是：

- 一个彻底开源的，面向企业应用开发的“大数据库集群”
- 支持事务、ACID、可以替代Mysql的加强版数据库
- 一个可以视为“Mysql”集群的企业级数据库，用来替代昂贵的Oracle集群
- 一个融合内存缓存技术、Nosql技术、HDFS大数据的新型SQL Server
- 结合传统数据库和新型分布式数据仓库的新一代企业级数据库产品
- 一个新颖的数据库中间件产品

MyCAT的目标是：低成本的将现有的单机数据库和应用平滑迁移到“云”端，解决数据存储和业务规模迅速增长情况下的数据瓶颈问题。

**MyCAT的关键特性：**

- 支持 SQL 92标准
- 支持 Mysql 集群，可以作为 Proxy 使用
- 支持 JDBC 连接 ORACLE、DB2、SQL Server，将其模拟为 MySQL Server 使用
- 支持 galera for mysql 集群，percona-cluster 或者 mariadb cluster，提供高可用性数据分片集群
- 自动故障切换，高可用性
- 支持读写分离，支持 Mysql 双主多从，以及一主多从的模式
- 支持全局表，数据自动分片到多个节点，用于高效表关联查询
- 支持独有的基于 E-R 关系的分片策略，实现了高效的表关联查询
- 多平台支持，部署和实施简单

**Mycat解决的问题：**

- 性能问题
- 数据库连接过多
- E-R分片难处理
- 可用性问题
- 成本和伸缩性问题

![x](./Resource/MyCat1.png)

**Mycat对多数据库的支持：**

![x](./Resource/MyCat3.png)

**Mycat的下载：**

- 官方网站：[http://www.mycat.org.cn/](http://www.mycat.org.cn/)
- github地址：[https://github.com/MyCATApache](https://github.com/MyCATApache)

**Mycat安装：**

- 第一步：把MyCat的压缩包上传到linux服务器
- 第二步：解压缩，得到mycat目录

  ```sh
  # windows环境下安装
  mycat.bat install
  ```

- 第三步：进入mycat/bin，启动MyCat
  
  ```sh
  # 启动命令：
  ./mycat start
  # 停止命令：
  ./mycat stop
  # 重启命令：
  ./mycat restart
  # 查看状态：
  ./mycat status
  ```

>注意：可以使用mysql的客户端直接连接mycat服务。默认服务端口为8066

### MyCAT架构

![x](./Resource/MyCat.png)

如图所示：MyCAT 使用 Mysql 的通讯协议模拟成了一个 Mysql 服务器，并建立了完整的Schema（数据库）、Table （数据表）、User(用户)的逻辑模型，并将这套逻辑模型映射到后端的存储节点DataNode（MySQL Instance）上的真实物理库中，这样一来，所有能使用 Mysql 的客户端以及编程语言都能将 MyCAT 当成是 Mysql Server 来使用，不必开发新的客户端协议。

### 分片策略

MyCAT支持水平分片与垂直分片：

- 水平分片：一个表格的数据分割到多个节点上，按照行分隔。
- 垂直分片：一个数据库中多个表格A，B，C，A存储到节点1上，B存储到节点2上，C存储到节点3上。

![x](./Resource/MyCat分片策略.png)

MyCAT 通过定义表的分片规则来实现分片，每个表格可以捆绑一个分片规则，每个分片规则指定一个分片字段并绑定一个函数，来实现动态分片算法。

1. Schema：逻辑库，与MySQL中的Database（数据库）对应，一个逻辑库中定义了所包括的Table。
2. Table：表，即物理数据库中存储的某一张表，与传统数据库不同，这里的表格需要声明其所存储的逻辑数据节点DataNode。在此可以指定表的分片规则。
3. DataNode：MyCAT的逻辑数据节点，是存放table的具体物理节点，也称之为分片节点，通过DataSource来关联到后端某个具体数据库上。DataSource：定义某个物理库的访问地址，用于捆绑到Datanode上

### 配置环境示例

1、mysql节点1环境：

```txt
操作系统版本 : centos6.4
数据库版本 : mysql-5.6
mycat版本 ：1.4 release
数据库名 : db1、db3
ip:192.168.25.134
```

2、mysql节点2环境

```txt
操作系统版本 : centos6.4
数据库版本 : mysql-5.6
mycat版本 ：1.4 release
数据库名 : db2
ip:192.168.25.166
```

MyCat安装到节点1上（需要安装jdk）

**配置schema.xml：**

Schema.xml 作为 MyCat 中重要的配置文件之一，管理着 MyCat 的逻辑库、表、分片规则、DataNode 以及DataSource。弄懂这些配置，是正确使用 MyCat 的前提。这里就一层层对该文件进行解析。

- schema 标签用于定义 MyCat 实例中的逻辑库
- Table 标签定义了 MyCat 中的逻辑表
- dataNode 标签定义了 MyCat 中的数据节点，也就是我们通常说所的数据分片。
- dataHost 标签在 mycat 逻辑库中也是作为最底层的标签存在，直接定义了具体的数据库实例、读写分离配置和心跳语句。

>注意：若是 LINUX 版本的 MYSQL，则需要设置为 Mysql 大小写不敏感，否则可能会发生表找不到的问题。

在 MySQL 的配置文件中my.ini [mysqld] 中增加一行：

```ini
lower_case_table_names = 1
```

Schema.xml配置示例：

```xml
<?xml version="1.0"?>
<!DOCTYPE mycat:schema SYSTEM "schema.dtd">
<mycat:schema xmlns:mycat="http://org.opencloudb/">
  <schema name="TESTDB" checkSQLschema="false" sqlMaxLimit="100">
    <!-- auto sharding by id (long) -->
    <table name="TB_ITEM" dataNode="dn1,dn2,dn3" rule="auto-sharding-long" />
    <table name="TB_USER" primaryKey="ID" type="global" dataNode="dn1,dn2" />
  </schema>
  <dataNode name="dn1" dataHost="localhost1" database="db1" />
  <dataNode name="dn2" dataHost="localhost2" database="db2" />
  <dataNode name="dn3" dataHost="localhost1" database="db3" />
  <dataHost name="localhost1" maxCon="1000" minCon="10" balance="0"
      writeType="0" dbType="mysql" dbDriver="native" switchType="1"  slaveThreshold="100">
    <heartbeat>select user()</heartbeat>
    <!-- can have multi write hosts -->
    <writeHost host="hostM1" url="192.168.25.134:3306" user="root" password="root">
      <!-- can have multi read hosts -->
    </writeHost>
  </dataHost>
  <dataHost name="localhost2" maxCon="1000" minCon="10" balance="0"
      writeType="0" dbType="mysql" dbDriver="native" switchType="1"  slaveThreshold="100">
    <heartbeat>select user()</heartbeat>
    <!-- can have multi write hosts -->
    <writeHost host="hostM1" url="192.168.25.166:3306" user="root" password="root">
      <!-- can have multi read hosts -->
    </writeHost>
  </dataHost>
</mycat:schema>
```

**配置server.xml：**

server.xml几乎保存了所有mycat需要的系统配置信息。最常用的是在此配置用户名、密码及权限。

```xml
<user name="test">
  <property name="password">test</property>
  <property name="schemas">TESTDB</property>
  <property name="readOnly">true</property>
</user>
```

**配置rule.xml：**

- rule.xml 里面就定义了我们对表进行拆分所涉及到的规则定义。我们可以灵活的对表使用不同的分片算法，或者对表使用相同的算法但具体的参数不同。
- 这个文件里面主要有 tableRule 和 function 这两个标签。在具体使用过程中可以按照需求添加 tableRule 和function。
- 此配置文件可以不用修改，使用默认即可。

分片测试：

- 由于配置的分片规则为 "auto-sharding-long"，所以 mycat 会根据此规则自动分片。
- 每个 datanode 中保存一定数量的数据。根据 id 进行分片
- 经测试 id 范围为：

  ```sh
  Datanode1：1~5000000
  Datanode2：5000000~10000000
  Datanode3：10000001~15000000
  ```

  当15000000以上的id插入时报错：

  ```txt
  [Err] 1064 - can't find any valid datanode :TB_ITEM -> ID -> 15000001
  ```

  此时需要添加节点了。

**Mycat读写分离：**

数据库读写分离对于大型系统或者访问量很高的互联网应用来说，是必不可少的一个重要功能。对于 MySQL 来说，标准的读写分离是主从模式，一个写节点 Master 后面跟着多个读节点，读节点的数量取决于系统的压力，通常是 1-3 个读节点的配置

![x](./Resource/MycatMS.png)

Mycat 读写分离和自动切换机制，需要 mysql 的主从复制机制配合。

**Mysql的主从复制：**

![x](./Resource/MysqlMS.png)

主从配置需要注意的地方：

1. 主DB server和从DB server数据库的版本一致
2. 主DB server和从DB server数据库数据一致（这里就会可以把主的备份在从上还原，也可以直接将主的数据目录拷贝到从的相应数据目录）
3. 主DB server开启二进制日志，主DB server和从DB server的server_id都必须唯一

**Mysql主服务器配置：**

- 第一步：修改my.conf文件：

  在[mysqld]段下添加：

  ```ini
  binlog-do-db=db1
  binlog-ignore-db=mysql
  # 启用二进制日志
  log-bin=mysql-bin
  # 服务器唯一ID，一般取IP最后一段
  server-id=134
  ```

- 第二步：重启mysql服务：`service mysqld restart`
- 第三步：建立帐户并授权slave

  ```sh
  mysql> GRANT FILE ON *.* TO 'backup'@'%' IDENTIFIED BY '123456';
  mysql> GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* to 'backup'@'%' identified by '123456';
  # 一般不用root帐号，"%"表示所有客户端都可能连，只要帐号，密码正确，此处可用具体客户端IP代替，如192.168.145.226，加强安全。
  
  # 刷新权限
  mysql> FLUSH PRIVILEGES;

  # 查看mysql现在有哪些用户

  mysql> select user,host from mysql.user;
  ```

- 第四步：查询master的状态
  
  ```sh
  mysql> show master status;
  ```

**Mysql从服务器配置：**

- 第一步：修改my.conf文件

  ```ini
  [mysqld]
  server-id=166
  ```

## 配置中心

### 基于zookeeper实现统一配置管理

**为什么需要统一配置？**

做项目时用到的配置比如数据库连接等最简单的方式是写死在项目里，如果需要更改，那么就通过修改配置文件然后再投产上去。问题来了，如果是集群呢，有100台机器，这时候做修改就太不切实际了；我们需要用统一配置管理。

**解决思路：**

1. 把公共配置抽取出来
2. 对公共配置进行维护
3. 修改公共配置后应用不需要重新部署

**采用Zookeeper方案：**

![x](./Resource/Zookeeper方案.png)

1. 公共配置抽取存放于zookeeper中并落地数据库
2. 对公共配置修改后发布到zookeeper中并落地数据库
3. 对应用开启配置实时监听，zookeeper配置文件一旦被修改，应用可实时监听到并获取

## Zookeeper

ZooKeeper 是一个开源的分布式协调服务，由雅虎创建，是 Google Chubby 的开源实现。

分布式应用程序可以基于 ZooKeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、配置维护，名字服务、分布式同步、分布式锁和分布式队列 等功能。

为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么 ZooKeeper 本身仍然是可用的。

客户端在使用 ZooKeeper 时，需要知道集群机器列表，通过与集群中的某一台机器建立 TCP 长连接来使用服务。客户端使用这个 TCP 链接来发送请求、获取结果、获取监听事件以及发送心跳包。如果这个连接异常断开了，客户端可以连接到另外的机器上。

![x](./Resource/Zookeeper.png)

上图是 ZooKeeper 官方提供的架构图：

- 图中每一个 Server 代表一个安装 ZooKeeper 服务的服务器。组成 ZooKeeper 服务的服务器都会在内存中维护当前的服务器状态，并且每台服务器之间都互相保持着通信。
- 集群间通过 Zab 协议（Zookeeper AtomicBroadcast）来保持数据的一致性。
- 对于来自客户端的每个更新请求，ZooKeeper 都会分配一个全局唯一的递增编号。这个编号反应了所有事务操作的先后顺序，应用程序可以使用 ZooKeeper 这个特性来实现更高层次的同步原语。这个编号也叫做时间戳—zxid（ZooKeeper Transaction Id）。

最典型集群模式：Master/Slave 模式（主备模式）。在这种模式中，通常 Master 服务器作为主服务器提供写服务，其他的 Slave 服务器从服务器通过异步复制的方式获取 Master 服务器最新的数据提供读服务。

但是，在 ZooKeeper 中没有选择传统的 Master/Slave 概念，而是引入了Leader、Follower 和 Observer 三种角色。如下图所示：

![x](./Resource/Zookeeper1.png)

- ZooKeeper 集群中的所有机器通过一个 Leader 选举过程来选定一台称为 "Leader" 的机器。
- Leader 既可以为客户端提供写服务又能提供读服务。除了 Leader 外，Follower 和  Observer 都只能提供读服务
- Server：Server中存在两种类型：Follower 和 Observer。其中 Follower 接受客户端的请求并返回结果（事务请求将转发给 Leader 处理），并在选举过程中参与投票；
- Observer 与 Follower 功能一致，唯一的区别在于 Observer 机器不参与 Leader 的选举过程，也不参与写操作的“过半写成功”策略，因此 Observer 机器可以在不影响写性能的情况下提升集群的读性能。
- Client：请求发起方，Server 和 Client 之间可以通过长连接的方式进行交互。如发起注册或者请求集群信息等。
- 集群间通过 Zab 协议（Zookeeper Atomic Broadcast）来保持数据的一致性。

### Zab协议

ZAB（ZooKeeper Atomic Broadcast 原子广播）协议是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。ZAB 协议包括两种基本的模式，分别是崩溃恢复和消息广播。

![x](./Resource/Zab协议.png)

- 当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进入恢复模式并选举产生新的 Leader 服务器。
- 当选举产生了新的 Leader 服务器，同时集群中已经有过半的机器与该 Leader 服务器完成了状态同步之后，ZAB 协议就会退出恢复模式。

其中，所谓的状态同步是指数据同步，用来保证集群中存在过半的机器能够和 Leader 服务器的数据状态保持一致。当集群中已经有过半的 Follower 服务器完成了和 Leader 服务器的状态同步，那么整个服务框架就可以进人消息广播模式了。

当一台同样遵守 ZAB 协议的服务器启动后加入到集群中时，如果此时集群中已经存在一个 Leader 服务器在负责进行消息广播。那么新加入的服务器就会自觉地进人数据恢复模式：找到 Leader 所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。

正如上文介绍中所说的，ZooKeeper 设计成只允许唯一的一个 Leader 服务器来进行事务请求的处理。Leader 服务器在接收到客户端的事务请求后，会生成对应的事务提案并发起一轮广播协议。而如果集群中的其他机器接收到客户端的事务请求，那么这些非 Leader 服务器会首先将这个事务请求转发给 Leader 服务器。

## Consul

Consul 是 HashiCorp 公司推出的开源工具，基于 Go 语言开发的支持多数据中心分布式高可用的服务发布和注册服务软件，采用 Raft 算法保证服务的一致性，且支持健康检查。

因为通过 Golang 实现，因此具有天然可移植性（支持Linux、windows和Mac OS X）；安装包仅包含一个可执行文件，方便部署，与 Docker 等轻量级容器可无缝配合。

Consul 采用主从模式的设计，使得集群的数量可以大规模扩展，集群间通过 RPC 的方式调用（HTTP 和 DNS）。它的结构图如下所示：

![x](./Resource/Consul.png)

- Client：作为一个代理（非微服务实例），它将转发所有的 RPC 请求到 Server 中。作为相对无状态的服务，它不持有任何注册信息。
- Server：作为一个具备扩展功能的代理，它将响应 RPC 查询、参与 Raft 选举、维护集群状态和转发查询给 Leader 等。
- Leader-Server：一个数据中心的所有 Server 都作为 Raft 节点集合的一部分。其中 Leader 将负责所有的查询和事务（如服务注册），同时这些事务也会被复制到所有其他的节点。
- Data Center：数据中心作为一个私有的，低延迟和高带宽的一个网络环境。每个数据中心会存在 Consul 集群，一般建议 Server 是 3-5 台（考虑到 Raft 算法在可用性和性能上取舍），而 Leader 只能唯一，Client 的数量没有限制，可以轻松扩展。Consul 通过 WAN 的 Gossip 协议，完成跨数据中心的同步；而且其他的产品则需要额外的开发工作来实现。

### Raft 算法

不同于 Paxos 算法直接从分布式一致性问题出发推导出来，Raft 算法则是从多副本状态机的角度提出，用于管理多副本状态机的日志复制。Raft 实现了和 Paxos 相同的功能，它将一致性分解为多个子问题：Leader 选举（Leader election）、日志同步（Log replication）、安全性（Safety）、日志压缩（Log compaction）、成员变更（Membership change）等。同时，Raft 算法使用了更强的假设来减少了需要考虑的状态，使之变的易于理解和实现。

![x](./Resource/Raft算法.png)

Raft 算法将 Server 分为三种类型：Leader、Follower 和 Candidate。

- Leader 处理所有的查询和事务，并向 Follower 同步事务。
- Follower 会将所有的 RPC 查询和事务转发给 Leader 处理，它仅从 Leader 接受事务的同步。数据的一致性以 Leader 中的数据为准实现。

在节点初始启动时，节点的 Raft 状态机将处于 Follower 状态等待来来自 Leader 节点的心跳。如果在一定时间周期内没有收到 Leader 节点的心跳，节点将发起选举。

Follower 节点选举时会将自己的状态切换为 Candidate，然后向集群中其它 Follower 节点发送请求，询问其是否选举自己成为 Leader。当收到来自集群中过半数节点的接受投票后，节点即成为 Leader，开始接收 Client 的事务处理和查询并向其它的 Follower 节点同步事务。Leader 节点会定时向 Follower 发送心跳来保持其地位。

### Gossip协议

Gossip protocol 也叫 Epidemic Protocol （流行病协议），实际上它还有很多别名，比如：“流言算法”、“疫情传播算法”等。

Gossip 过程是由种子节点发起，当一个种子节点有状态需要更新到网络中的其他节点时，它会随机的选择周围几个节点散播消息，收到消息的节点也会重复该过程，直至最终网络中所有的节点都收到了消息。

这个过程可能需要一定的时间，由于不能保证某个时刻所有节点都收到消息，但是理论上最终所有节点都会收到消息，因此它是一个最终一致性协议。这个协议的作用就像其名字表示的意思一样，非常容易理解，它的方式其实在我们日常生活中也很常见，比如电脑病毒的传播，森林大火，细胞扩散等等。

![x](./Resource/Gossip协议.png)

为了表述清楚，我们先做一些前提设定：

1. Gossip 是周期性的散播消息，把周期限定为 1 秒
2. 被感染节点随机选择 k 个邻接节点（fan-out）散播消息，这里把 fan-out 设置为 3，每次最多往 3 个节点散播。
3. 每次散播消息都选择尚未发送过的节点进行散播
4. 收到消息的节点不再往发送节点散播，比如 A -> B，那么 B 进行散播的时候，不再发给 A。

这里一共有 16 个节点，节点 1 为初始被感染节点，通过 Gossip 过程，最终所有节点都被感染。

**Gossip 的特点（优势）：**

1）**扩展性**

网络可以允许节点的任意增加和减少，新增加的节点的状态最终会与其他节点一致。

2）**容错**

网络中任何节点的宕机和重启都不会影响 Gossip 消息的传播，Gossip 协议具有天然的分布式系统容错特性。

3）**去中心化**

Gossip 协议不要求任何中心节点，所有节点都可以是对等的，任何一个节点无需知道整个网络状况，只要网络是连通的，任意一个节点就可以把消息散播到全网。

4）**一致性收敛**

Gossip 协议中的消息会以一传十、十传百一样的指数级速度在网络中快速传播，因此系统状态的不一致可以在很快的时间内收敛到一致。消息传播速度达到了 logN。

5）**简单**

Gossip 协议的过程极其简单，实现起来几乎没有太多复杂性。

**Gossip 的缺陷：**

分布式网络中，没有一种完美的解决方案，Gossip 协议跟其他协议一样，也有一些不可避免的缺陷，主要是两个：

1）**消息的延迟**

由于 Gossip 协议中，节点只会随机向少数几个节点发送消息，消息最终是通过多个轮次的散播而到达全网的，因此使用 Gossip 协议会造成不可避免的消息延迟。不适合用在对实时性要求较高的场景下。

2）**消息冗余**

Gossip 协议规定，节点会定期随机选择周围节点发送消息，而收到消息的节点也会重复该步骤，因此就不可避免的存在消息重复发送给同一节点的情况，造成了消息的冗余，同时也增加了收到消息的节点的处理压力。而且，由于是定期发送，因此，即使收到了消息的节点还会反复收到重复消息，加重了消息的冗余。

Gossip 协议是为了解决分布式环境下监控和事件通知的瓶颈。

Gossip 协议中的每个 Agent 会利用 Gossip 协议互相检查在线状态，分担了服务器节点的心跳压力，通过 Gossip 广播的方式发送消息。所有的 Agent 都运行着 Gossip 协议。服务器节点和普通 Agent 都会加入这个 Gossip 集群，收发 Gossip 消息。

每隔一段时间，每个节点都会随机选择几个节点发送 Gossip 消息，其他节点会再次随机选择其他几个节点接力发送消息。这样一段时间过后，整个集群都能收到这条消息。基于 Raft 算法，Consul 提供强一致性的注册中心服务，但是由于 Leader 节点承担了所有的处理工作，势必加大了注册和发现的代价，降低了服务的可用性。

通过 Gossip 协议，Consul 可以很好地监控 Consul 集群的运行，同时可以方便通知各类事件，如 Leader 选择发生、Server 地址变更等。

## 服务注册中心选型

下面是这三个服务注册中心的特性对比， 没有最好的服务中心， 只有最合适的，我们可以根据项目的实际情况来进行选择：

![x](./Resource/服务注册中心选型.png)

**为什么不使用Zookeeper作为服务注册表？**

1. zookeeper在写入操作必须得到法定人数确认才能成功提交。
2. zookeeper在选举期间会停止对外服务

选Eureka理由：（[参考](https://github.com/Netflix/eureka/wiki/Understanding-Eureka-Peer-to-Peer-Communication)）

1. Eureka 客户试图与 Eureka Server 交互。如果与服务器通信时出现问题或者同一区域中不存在服务器，则客户端将故障转移到其他区域中的服务器。

2. 一旦服务器开始接收流量，服务器上执行的所有操作都将复制到服务器知道的所有对等节点。如果某个操作由于某种原因而失败，则会在下一个也会在服务器之间复制的心跳上协调该信息。
